\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\newcommand{\del}{\nabla}
\begin{document}

\title{Kronecker Graphs for Modelling Linguistic Data}

\section{Introduction}

An important statistical regularity in language is Zipf's law, which says that given some corpus of natural language, the frequency of any word is inversely proportional to its rank on the frequency table. However remarkable it may be, however, it is not a deep statement about the organization of language. This is because it holds for corpuses as bags of words which have frequencies, not as linguistic structures. %%%%%% get refs for "not a deep statment"

One important language model which is used in many places which incorporates some aspect of linguistic structure is the n-gram, which creates a Markov model of language, taking the semi-Markov assumption that the only context needed to reproduce a word is the past $n$ words. This is of interest, because it turns out that thinking of a corpus as a graph of words and the order-one relations between words, as a bigram model does, reveals many small-world properties in the structuring of words in corpuses. %%% get ref for n-gram, use the small world of language ref

In this paper we deal, therefore, with the problem of using and adapting already existing algorithms for creating small world nets to creating a parsimonious model of language that respects these network properties of words as they currently exist.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%555
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%555
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%555
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%555

\section{Literature Review}

MEJ Newman power laws paper, Kronecker intro paper, Small World of Human Language paper

\subsection{Summary}
%What was the main technical content of the papers?
MEJ power laws paper says that you should graph with CDFs, talk about the informational meaning of words, many possible origins of power law phenomena. Mostly about critical phenomena, which is a philosophical debate and not within the scope of the class.

Kronecker intro paper says do a fractal on the adjacency matrix, construed as a picture. There are these long list of properties, heavy tail for indegree, heavy tail for outdegree, densification power law, small diameters. Note that it's not searchable in this case. Also a good resource for looking at all the properties of the small world net canonically, because they really expect the model to fit all of them

Small world of human language paper of this shit. Says that there is a complex system. I put forward all the data alongside this.

%How do papers relate to the topics presented in the course?
They invoke many small-world properties at the same time and are therefore great models for a variety of natural networks. But the current literature has focused upon social networks, but there are many possible objects of network analysis. I want to explore one which doesn't appear much in the literature, which is the relation words have to each other. Instead of creating semantic nets, I want to explore the ones which are implicit in bigram models or co-occurance nets.

\subsection{Discussion} %%% critique
%What are strengths and weaknesses of the papers and how they are addressed?
%What were the authors missing?
MEJ: No grammar, so the vocabulary and the grammar may be separate.

Other people have noted the lack of local search in stochastic kronecker graphs. This shouldn't have any problem, but think about local search in semantic networks and it seems to happen.
%Was anything particularly unrealistic?

Small world of human language should have recognized the practical implications for language modelling. Connected words consideration: long-distance correlations, they give reasons but I would actually give another reason that the long-distnace stuff could be frequent paths in the language of the graph

They use the BA model, but at the smae time they like the diameter, which does show log log n behavior, and the clustering coefficient, which is lower than the BA model.

%What are promising further research questions in the direction of the papers?
%How could they be pursued?
%An idea of a better model for something? A better algorithm? A test of an algo or model on a dataset?

Use the Kronecker model for datasets. Not multifractal, mainly because implementation itself can become too heavy. If it's a good kronecker model, is it related to all these other algorithms which can use a bigram as a language model? I mainly have a problem with the characterization of language as unstructured: only a first approximation.

Perhaps one of the properties which lie un-predicted but are good targets for a Kronecker graph are the language modelling properties and the proper distributions and stuff.

\section{Project Proposal}

\subsection{Introduction}
\subsection{Data and Problem}
\subsection{Network Representation}
\subsection{Language Model}
\subsection{Evaluation}
%% test of a model or algorithm on a dataset or on simulated data
Do the kronecker fit on a bigram model to make language model
%% a proposal for a model or algorithm that potentially extends or improves the topics discussed in the papers
Use it as a language model for MT, I don't know
I know what I'm doing, but what are you?
Maybe sample from it for language modelling

Put in all the data that I have

%%%
%%% PUT IN CITATIONS
%%%

\end{document}
