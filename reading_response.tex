\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[margin=1in]{geometry}
\newcommand{\del}{\nabla}
\begin{document}

\title{Kronecker Graphs for Modelling Linguistic Data}
\author{Howon Lee}
\maketitle

\section{Introduction}

An important statistical regularity in language is Zipf's law, which says that given some corpus of natural language, the frequency of any word is inversely proportional to its rank on the frequency table. However remarkable it may be, however, it is not a deep statement about the organization of language. This is because it holds for corpuses as bags of words which have frequencies, not as linguistic entities which have internal structure. %%%%%% get refs for "not a deep statment"

One important language model which is used in many places which incorporates some aspect of linguistic structure is the n-gram, which creates a Markov model of language, taking the semi-Markov assumption that the only context needed to reproduce a word is the past $n$ words. This is of interest, because it turns out that thinking of a corpus as a graph of words and the order-one and order-two relations between words, as bigram and trigram models do, reveals many small-world properties in the structuring of words in corpuses. %%% get ref for n-gram, use the small world of language ref

In this paper we deal, therefore, with the problem of using and adapting already existing algorithms for creating small world nets by creating a parsimonious model of language with Kronecker graphs that respects these network properties of words as they currently exist.

\section{Literature Review}

We review MEJ Newman's \emph{Power laws, Pareto Distributions and Zipf's law}, J. Leskovec and C. Faloutsos's \emph{Scalable Modeling of Real Graphs using Kronecker Multiplication}, and RF Cancho and RV Sole's \emph{The Small World of Human Language}. %%%% get refs for all of them

\subsection{MEJ Newman}
MEJ Newman's paper is a review and introduction to power laws, which are notable for being found in very many domains across nature, and for the unhelpfulness of their first and second moments. That is, the number of inhabitants of the mean city is on the order of $10^3$, but that doesn't tell you about the existence of Tokyo. There are many pitfalls in the estimation of power laws, especially in the least mean square estimation, which gets systematic errors even on synthetic data, and many possible origins of power law phenomena, which are listed: two important ones are Yule processes and critical phenomena.

A particular point of interest is the introduction to a possible information-theoretic origin of Zipf's law. MEJ Newman reviews a possible origin of power law phenomena in language due to Miller. Miller's argument goes like this: imagine a monkey typing on a typewriter, delimiting words with some probability and writing a random letter with some uniform probability. Then, this approximates an exponential distribution of frequency of a word $x$ with number of letters $y$. But the number of possible words goes up exponentially with $y$ also, making the distribution of frequencies a combination of exponentials, which ends up being a power law. This is a simplistic argument, but can be made less simplistic in talking about arbitrary information in bits instead of symbols in the normal alphabet. %%%%% cite miller
. 
\subsection{MEJ Newman Discussion}

As a review paper, there is not any original work in the paper: it must be criticized in whether the distribution of subjects in the review cleave closely to the importance the field should give to the respective subjects. To that end, a strong point of the paper is its covering most things which were known about the power law phenomena at the time, and being skeptical about the general trend of plotting some distribution, finding a fat tail, and declaring that the phenomenon respected a power law: that skepticism is continued in C. Shalzi's paper, and will be used in our fractal analysis. %%%% cite cosmo

The main point of contention we have with the paper is in covering as the purported source of Zipf's law GA Miller's monkey on a typewriter argument. Because this theory is only compatible with a random typewriter with no structure. That is, to condition on the previous letters or words gives no information for the generation of the typewriter model, except that the example of Shannon's communication game, that of guessing the next word based upon the previous words in a communication, implies this sort of conditional structure. And humans in real life can play this communication game. This is probably more of a contention with Miller's paper than with MEJ Newman's.

\subsection{J Leskovec, C. Faloutsos}
A possible model of small world graphs is the stochastic Kronecker graph, which is a sample from a distribution over graphs. The distribution is created by drawing a fractal over an adjacency matrix using the Kronecker product operation. The important thing about this model is that it tries to simultaneously fulfill the many observed properties of small world graphs, not just the heavy tail for indegree and outdegree, but also the heavy tail for scree plots, the densification power law, and small diameters, all at the same time.

The novel contribution of this paper is to note that the parameters for that distribution can be fitted with an algorithm described in the paper. KronFit is a gradient descent algorithm, but is much, much faster than other such algorithms for graphs like the exponential random graph. This is because it uses MCMC to assign node labels, avoiding that combinatorial explosion, and because it uses the sparsity of the small world graph to start with the likelihood of an empty graph and adjust for the edges that exist, in time linear to the number of edges instead of quadratic to the number of nodes. The number of parameters is determined using a BIC metric. Every part of this algorithm can and should be done when the whole matrix does not fit in memory.

There are also real interpretations of the Kronecker product process in the structure and creation of graphs. One intuition is that networks are hierarchically organized into communities, which grow recursively, creating miniature copies of themselves. Another interpretation is to say that each node is described by a sequence of categorical features, and the probability of two nodes linking depends on the product of individual attribute similarities, which allows modelling of homophily and heterophily at the same time.

\subsection{J Leskovec, C. Faloutsos discussion}

One important point to re-iterate is that one suggested usage for the Kronecker fitting algorithm is for compression of graphs. This is important, because whenever we hear "compression" we should smell "machine learning", because any system of compression can be used for prediction, by finding the symbol that compresses best given the history of the data.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%What are strengths and weaknesses of the papers and how they are addressed?
%What were the authors missing?
%Was anything particularly unrealistic?

Other people have noted that there is not a capacity for local search in stochastic kronecker graphs. %% will this matter? will this not matter?

No mention of the fact that the construction itself is fractal, we can get some leverage out of that I think

\subsection{RF Cancho, RV Sole}
Cancho and Sole's paper talk about the implicit statistical regularities in a small world net made from the co-occurrence network of words in sentences. Instead of talking about syntactic structure in sentences, they investigate the statistical structure of these mere co-occurrences, with the admittedly simplistic assumption that the trigrams and bigrams are correlated.

Why? Four reasons. It's much easier to get those link structures automatically, as opposed to syntactic ones. They don't know what types of links there are in the structures of words, but just having the propinquity of words will capture almost every type. They are not interested in all the links, so looking at whole sentences will not be as productive. Long-distance syntactic links imply the existence of lower-distance syntactic links, but not vice versa. %%% cite for propinquity, cite for long-distance syntactic links implying existence of lower-distnace syntactic links but not vice versa

Looking at the graph created in this way, they find small-world properties like high clustering coefficient, small diameter and power law degree distribution. They hypothesize that this leads to words that exist to speed up navigation in this small world, words like "and", "the", "of", "in", which do not contribute meaning but structure to grammar. They also hypothesize that the disfluency caused in agrammatism is caused by disruption to this small world.

\subsection{RF Cancho, RV Sole discussion}

%%% strengths, weaknesses, how are they addressed?
There is ample room for criticizing the decision to say that two words are linked to each other if they are within two words of each other in a corpus. %%%%%%%% withering criticism, friends

%%% what are they missing
Missing a practical application for the language model, because unlike in other fields creating a model for this sampling becomes useful. We know that humans solve the inverse problem for languages in some way, because there has to be teaching for the learning of a specific language, if not for the learning of language in the first place, if you believe in Universal Grammar. However, this is not true in many other power-law distributed domains: \emph{Only fools, charlatans and liars predict earthquakes} (Richter). Therefore, there should be room for usage as a language model. %%% cite for richter, chomsky

%Was anything particularly unrealistic?
They use the BA model, but at the smae time they like the diameter, which does show log log n behavior, and the clustering coefficient, which is lower than the BA model. Criticize the null model a bit more, but note the time period.

\subsection{Brainstorm}
%What are promising further research questions in the direction of the papers?
%How could they be pursued?
%An idea of a better model for something? A better algorithm? A test of an algo or model on a dataset?

Use the Kronecker model for datasets. Not multifractal, mainly because implementation itself can become too heavy. If it's a good kronecker model, is it related to all these other algorithms which can use a bigram as a language model? I mainly have a problem with the characterization of language as unstructured: only a first approximation.

Perhaps one of the properties which lie un-predicted but are good targets for a Kronecker graph are the language modelling properties and the proper distributions and stuff.

\section{Project Proposal}

\subsection{Introduction}
We will fit a Kronecker graph generator on a n-gram model of a corpus. We will see the generated Kronecker graph's usefulness as a very parsimonious language model on its own.
\subsection{Data}
Medium-sized and large corpuses of words are fairly easy to get. We will use the Brown NLTK corpus for development, and then take advantage of the many corpuses in NLTK to attempt the authorship measure, in particular the Gutenberg corpus. When we get to machine translation, we will use the Canadian Hansards bitext.
\subsection{Problem}
We will fit a Kronecker graph generator's parameters on an n-gram model to make a language model. This fits very well with already existing AI tasks, since language models are a technology used in many places in NLP.

We will also look at the fractal nature of n-gram models to use as a simple feature in an authorship attribution (classification) task.
\subsection{Evaluation}
Perplexity is an easy development measure of the generator's goodness. Of course, it's a bad approximation of anything we care about in the generalization performance. We should therefore also do two simple tasks with the language model, like generation itself and spell-checking, and then rig up an OCR system with this language model as the respective language model. What will then be compared is those three systems' generalization performance on a test set.

There may also be an interesting investigation possible of fractal dimension and lacunarity in the data itself. That is, implicit in the Kronecker graph model is an assumption that the network's adjacency matrix can be treated as a fractal: we should run as far as we can with this assumption, and see if fractal measures of this kind are meaningful and useful as features of the corpus. To this end, we will test an authorship attribution task with standard machine learning algorithms: the specific machine learning algorithm in this case should be less important than ascertaining the fact of whether fractal dimension and lacunarity are important features.

%%%
%%% PUT IN CITATIONS
%%%

\end{document}
