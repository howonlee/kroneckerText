\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[margin=1in]{geometry}
\newcommand{\del}{\nabla}
\begin{document}

\title{Kronecker Graphs for Modelling Linguistic Data}

\section{Introduction}

An important statistical regularity in language is Zipf's law, which says that given some corpus of natural language, the frequency of any word is inversely proportional to its rank on the frequency table. However remarkable it may be, however, it is not a deep statement about the organization of language. This is because it holds for corpuses as bags of words which have frequencies, not as linguistic entities which have internal structure. %%%%%% get refs for "not a deep statment"

One important language model which is used in many places which incorporates some aspect of linguistic structure is the n-gram, which creates a Markov model of language, taking the semi-Markov assumption that the only context needed to reproduce a word is the past $n$ words. This is of interest, because it turns out that thinking of a corpus as a graph of words and the order-one relations between words, as a bigram model does, reveals many small-world properties in the structuring of words in corpuses. %%% get ref for n-gram, use the small world of language ref

In this paper we deal, therefore, with the problem of using and adapting already existing algorithms for creating small world nets by creating a parsimonious model of language with Kronecker graphs that respects these network properties of words as they currently exist.

\section{Literature Review}

We review MEJ Newman's \emph{Power laws, Pareto Distributions and Zipf's law}, J. Leskovec and C. Faloutsos's \emph{Scalable Modeling of Real Graphs using Kronecker Multiplication}, and RF Cancho and RV Sole's \emph{The Small World of Human Language}. %%%% get refs for all of them

\subsection{Summary}
\subsubsection{MEJ Newman}
MEJ Newman's paper is a review and introduction to power laws, which are notable for being found in very many domains across nature, and for the unhelpfulness of their first and second moments. That is, the number of inhabitants of an average city is on the order of $10^3$, but that doesn't tell you about the existence of Tokyo. There are many pitfalls in the estimation of power laws, especially in the least mean square estimation, which gets systematic errors even on synthetic data, and many possible origins of power law phenomena. Two important ones are Yule processes, which are stochastic processes where the Matthew effect (the winner wins more, the loser loses more) predominates, and criticality effects.

A particular point of interest is the introduction to a possible information-theoretic origin of Zipf's law. MEJ Newman reviews a possible origin of power law phenomena in language due to Miller. Miller's argument goes like this: imagine a monkey typing on a typewriter, delimiting words with some probability and writing a random letter with some uniform probability. Then, this approximates an exponential distribution of frequency of a word $x$ with number of letters $y$. But the number of possible words goes up exponentially with $y$ also, making the distribution of frequencies a combination of exponentials, which ends up being a power law. This is a simplistic argument, but can be made less simplistic in talking about arbitrary information in bits instead of symbols in the normal alphabet. %%%%% cite miller
. 

\subsubsection{J Leskovec, C. Faloutsos}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Kronecker intro paper says do a fractal on the adjacency matrix, construed as a picture. There are these long list of properties, heavy tail for indegree, heavy tail for outdegree, densification power law, small diameters. Note that it's not searchable in this case. Also a good resource for looking at all the properties of the small world net canonically, because they really expect the model to fit all of them
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{RF Cancho, RV Sole}
Cancho and Sole's paper talk about the implicit statistical regularities in a small world net made from the co-occurrence network of words in sentences. Instead of talking about syntactic structure in sentences, they investigate the statistical structure of these mere co-occurrences, with the admittedly simplistic assumption that the trigrams and bigrams are correlated.

Why? Four reasons:

\begin{enumerate}
  \item It's much easier to get those link structures automatically
  \item We don't know what types of links there are in the structures of words, but just having the propinquity of words will capture almost every type. %%% cite for propinquity
  \item We are not interested in all the links, so looking at whole sentences will not be as productive.
  \item Long-distance syntactic links imply the existence of lower-distance syntactic links, but not vice versa. %%%% cite
\end{enumerate}

Looking at the graph created in this way, they find small-world properties like high clustering coefficient, small diameter and power law degree distribution. They hypothesize that this leads to words that exist to speed up navigation in this small world, words like "and", "the", "of", "in", which do not contribute meaning but structure to grammar. They also hypothesize that the disfluency caused in agrammatism is caused by disruption to this small world.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%555
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%555
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%555
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%555

\subsection{Discussion} %%% critique
%What are strengths and weaknesses of the papers and how they are addressed?
%What were the authors missing?
MEJ: No grammar, so the vocabulary and the grammar may be separate.

Other people have noted the lack of local search in stochastic kronecker graphs. This shouldn't have any problem, but think about local search in semantic networks and it seems to happen.
%Was anything particularly unrealistic?

Small world of human language should have recognized the practical implications for language modelling. Connected words consideration: long-distance correlations, they give reasons but I would actually give another reason that the long-distnace stuff could be frequent paths in the language of the graph

They use the BA model, but at the smae time they like the diameter, which does show log log n behavior, and the clustering coefficient, which is lower than the BA model.

%What are promising further research questions in the direction of the papers?
%How could they be pursued?
%An idea of a better model for something? A better algorithm? A test of an algo or model on a dataset?

Use the Kronecker model for datasets. Not multifractal, mainly because implementation itself can become too heavy. If it's a good kronecker model, is it related to all these other algorithms which can use a bigram as a language model? I mainly have a problem with the characterization of language as unstructured: only a first approximation.

Perhaps one of the properties which lie un-predicted but are good targets for a Kronecker graph are the language modelling properties and the proper distributions and stuff.

\section{Project Proposal}

\subsection{Introduction}
We will fit a Kronecker graph generator on a bigram model of a corpus. We will see the generated Kronecker graph's usefulness as a language model on its own.
\subsection{Data and Problem}
Large corpuses are very easy to get. %%% more actual details plox
\subsection{Language Model}
\subsection{Evaluation}
%% test of a model or algorithm on a dataset or on simulated data
Do the kronecker fit on a bigram model to make language model. This fits very well with already existing AI tasks, so we should use it for one of these tasks, MT, for the prior.

We should see if these model long-range dependencies
%% a proposal for a model or algorithm that potentially extends or improves the topics discussed in the papers
Use it as a language model for MT, I don't know
I know what I'm doing, but what are you?
Maybe sample from it for language modelling

Put in all the data that I have

Perplexity is an easy intermediate measure, of course it's a bad approximation

There may also be interesting investigation of fractal dimension and other fractal measures like lacunarity

%%%
%%% PUT IN CITATIONS
%%%

\end{document}
